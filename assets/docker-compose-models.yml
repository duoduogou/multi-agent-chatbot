#
# SPDX-FileCopyrightText: Copyright (c) 1993-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
x-build-base: &build-base
  build:
    context: .
    dockerfile: Dockerfile.llamacpp
  image: local/llama.cpp:server-cuda

services:
  # Main Chat Model (120B)
  gpt-oss-120b:
    <<: *build-base
    container_name: gpt-oss-120b
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: 
      - "-m"
      - "/models/openai_gpt-oss-120b-Q4_K_M-00001-of-00002.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "16384"
      - "-n"
      - "8192"
      - "--n-gpu-layers"
      - "999"
      - "--jinja"
      - "--cont-batching"

  # Coder Model (1.5B)
  qwen-coder-1.5b:
    <<: *build-base
    container_name: qwen-coder-1.5b
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: 
      - "-m"
      - "/models/qwen2.5-coder-1.5b-instruct-q4_k_m.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-n"
      - "4096"
      - "--n-gpu-layers"
      - "999"
      - "--jinja"

  # VL Model (2B)
  qwen-vl-2b:
    <<: *build-base
    container_name: qwen-vl-2b
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Note: llama.cpp server for VL requires mmproj file if available/needed
    command: 
      - "-m"
      - "/models/Qwen2-VL-2B-Instruct-Q4_K_M.gguf"
      - "--mmproj"
      - "/models/mmproj-Qwen2-VL-2B-Instruct-f16.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-n"
      - "4096"
      - "--n-gpu-layers"
      - "999"
      - "--jinja"

  qwen3-embedding:
    <<: *build-base
    container_name: qwen3-embedding
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: 
      - "-m"
      - "/models/Qwen3-Embedding-4B-Q8_0.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "--jinja"
      - "--embeddings"

volumes:
  ollama-data:
  
networks:
  default:
    name: chatbot-net